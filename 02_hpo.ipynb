{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T14:31:30.387177Z",
     "start_time": "2020-05-22T14:31:29.910912Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T14:32:31.352355Z",
     "start_time": "2020-05-22T14:32:31.344835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://p10a117.pbm.ihost.com:8643/platform/rest/conductor/v1\n",
      "https://p10a117.pbm.ihost.com:9243/platform/rest/deeplearning/v1\n"
     ]
    }
   ],
   "source": [
    "# Environment details:\n",
    "\n",
    "\n",
    "protocol = 'https'\n",
    "\n",
    "master_host = 'p10a117.pbm.ihost.com'\n",
    "\n",
    "dli_rest_port = '9243'\n",
    "sc_rest_port = '8643'\n",
    "\n",
    "\n",
    "sc_rest_url =  protocol+'://'+master_host+':'+sc_rest_port+'/platform/rest/conductor/v1'\n",
    "dl_rest_url = protocol+'://'+master_host+':'+dli_rest_port+'/platform/rest/deeplearning/v1'\n",
    "\n",
    "print (sc_rest_url)\n",
    "print (dl_rest_url)\n",
    "# User login details\n",
    "\n",
    "wmla_user = 'vanstee'\n",
    "wmla_pwd = 'pwvanstee'\n",
    "\n",
    "\n",
    "myauth = (wmla_user, wmla_pwd)\n",
    "\n",
    "# Spark instance group details\n",
    "sigName = 'b0p036a-dliauto'\n",
    "\n",
    "# REST call variables\n",
    "commonHeaders = {'Accept': 'application/json'}\n",
    "\n",
    "\n",
    "#startTuneUrl='%s://%s:%s/platform/rest/deeplearning/v1/hypersearch' % (protocol, master_host, dli_rest_port)\n",
    "#sc_rest_url ='%s://%s:%d/platform/rest/conductor/v1' % (protocol, hostname, conductorport)\n",
    "\n",
    "req = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any existing hpo tasks and also verify the platform health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest API: **GET platform/rest/deeplearning/v1/hypersearch**\n",
    "- Description: Get all the hpo task that the login user can access.\n",
    "- OUTPUT: A list of hpo tasks and each one with the same format which can be found in the api doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T14:33:05.986759Z",
     "start_time": "2020-05-22T14:33:05.783411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getTuneStatusUrl: https://p10a117.pbm.ihost.com:9243/platform/rest/deeplearning/v1/hypersearch\n",
      "Hpo task: kelvinl-hpo-254313202512616, State: FINISHED\n",
      "Hpo task: kelvinl-hpo-254758923243594, State: FINISHED\n",
      "Hpo task: kelvinl-hpo-266345130757669, State: FAILED\n",
      "Hpo task: kelvinl-hpo-267216444279712, State: FINISHED\n",
      "Hpo task: kelvinl-hpo-403697921838806, State: FAILED\n",
      "Hpo task: kelvinl-hpo-404372545448297, State: FAILED\n",
      "Hpo task: kelvinl-hpo-404624765236135, State: FAILED\n",
      "Hpo task: kelvinl-hpo-405300918575393, State: FAILED\n",
      "Hpo task: kelvinl-hpo-405544795318004, State: FINISHED\n",
      "Hpo task: kelvinl-hpo-406348140701279, State: FINISHED\n",
      "Hpo task: kelvinl-hpo-410153859499985, State: FINISHED\n"
     ]
    }
   ],
   "source": [
    "getTuneStatusUrl = dl_rest_url + '/hypersearch'\n",
    "print ('getTuneStatusUrl: %s' %getTuneStatusUrl)\n",
    "r = req.get(getTuneStatusUrl, headers=commonHeaders, verify=False, auth=myauth)\n",
    "\n",
    "if not r.ok:\n",
    "    print('check hpo task status failed: code=%s, %s'%(r.status_code, r.content))\n",
    "else:\n",
    "    if len(r.json()) == 0:\n",
    "        print('There is no hpo task been created')\n",
    "    for item in r.json():\n",
    "        print('Hpo task: %s, State: %s'%(item['hpoName'], item['state']))\n",
    "        #print('Best:%s'%json.dumps(item.get('best'), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch a HPO task\n",
    "\n",
    "TODO : Explain the high level ..\n",
    "train mnist using WMLA HPO through API ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model file update to Run HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model changes required from 2 perspective:\n",
    "- Inject hyper-parameters for the sub-training during search\n",
    "- Retrieve sub-training result metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model update part 1 - Inject hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameters will be supplied in a file called **config.json** with JSON format,located in the current working directory and can be read direcly as the following example snippet.\n",
    "\n",
    "<pre>\n",
    "hyper_params = json.loads(open(\"<b>config.json</b>\").read())\n",
    "learning_rate = float(hyper_params.get(\"<b>learning_rate</b>\", \"0.01\"))\n",
    "</pre>\n",
    "\n",
    "After this, you can use these hyper-parameters during the model trainings. The **hyper-parameter name** and **value** type is defined through the search space part in body of REST call when launching a new hpo task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model update part 2 - Retrieve sub-training result metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of your training run, your code will need to create a file called **val_dict_list.json** with test metrics generated during training. These metrics will be used by the search algorithm to propose new sets of hyper-parameters. Please note that **val_dict_list.json** should be created under the result directory which can be retrieved through the environment variable **RESULT_DIR**.\n",
    "\n",
    "<pre>\n",
    "with open('{}/val_dict_list.json'.format(os.environ['<b>RESULT_DIR</b>']), 'w') as f:\n",
    "    json.dump(test_metrics, f)\n",
    "</pre>\n",
    "\n",
    "The content of **val_dict_list.json** will be some thing as below, **step** is some thing optional meaning the training iteration or epochs, one of **loss** and **accuracy** can be the name of target metric to optimize, at least one metric need to be included here. The specific name of metric used to optimize (minimize or maximize) is defined in the body of REST call when launching a new hpo task. \n",
    "\n",
    "```\n",
    "[\n",
    "{‘step’: 1, ‘loss’:0.2487, ‘accuracy’: 0.4523},\n",
    "{‘step’: 2, ‘loss’:0.1487, ‘accuracy’: 0.5523},\n",
    "{‘step’: 3, ‘loss’:0.1087, ‘accuracy’: 0.6523},\n",
    "…\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch HPO task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST API: **POST /platform/rest/deeplearning/v1/hypersearch**\n",
    "- Description: Start a new HPO task\n",
    "- Content-type: Multi-Form\n",
    "- Multi-Form Data:\n",
    "  - files: Model files tar package, ending with `.modelDir.tar`\n",
    "  - form-filed: {‘data’: ‘String format of input parameters to start hpo task, let’s call it as **hpo_input** and show its specification later’}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model file update to Run HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Package model files for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package the updated model files into a tar file ending with `.modelDir.tar`\n",
    "\n",
    "REST API expects a modelDir.tar with the model code inside ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T14:48:03.132330Z",
     "start_time": "2020-05-22T14:48:03.121855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tempFile: /tmp/tmp13240_og.modelDir.tar\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "import os\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "MODEL_DIR_SUFFIX = \".modelDir.tar\"\n",
    "tempFile = tempfile.mktemp(MODEL_DIR_SUFFIX)\n",
    "make_tarfile(tempFile, '/gpfs/software/wmla-p10a117/dli_data_fs/models/pytorch_hpo')\n",
    "print(\" tempFile: \" + tempFile)\n",
    "files = {'file': open(tempFile, 'rb')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct POST request data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hpo_input** will be a Python dict or json format as below, convert to string when calling REST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T14:48:05.655197Z",
     "start_time": "2020-05-22T14:48:05.642751Z"
    }
   },
   "outputs": [],
   "source": [
    "data =  {\n",
    "        'modelSpec': # Define the model training related parameters\n",
    "        {\n",
    "            # Spark instance group which will be used to run the HPO sub-trainings. The Spark instance group selected\n",
    "            # here should match the sub-training args, for example, if the sub-training args try to run a EDT job,\n",
    "            # then we should put a Spark instance group with capability to run EDT job here.\n",
    "            'sigName': sigName,\n",
    "\n",
    "            # These are the arguments we'll pass to the execution engine; they follow the same conventions\n",
    "            # of the dlicmd.py command line launcher\n",
    "            #\n",
    "            # See:\n",
    "            #   https://www.ibm.com/support/knowledgecenter/en/SSFHA8_1.2.1/cm/dlicmd.html\n",
    "            # In this example, args after --model-dir are all the required parameter for the original model itself.\n",
    "            #\n",
    "            'args': '--exec-start PyTorch --cs-datastore-meta type=fs --python-version 3.6\\\n",
    "                     --gpuPerWorker 1 --model-main pytorch_mnist_HPO.py --model-dir pytorch_hpo\\\n",
    "                     --debug-level debug'\n",
    "                \n",
    "        },\n",
    "    \n",
    "        'algoDef': # Define the parameters for search algorithms\n",
    "        {\n",
    "            # Name of the search algorithm, one of Random, Bayesian, Tpe, Hyperband, ExperimentGridSearch\n",
    "            'algorithm': 'Random', \n",
    "            # Max running time of the hpo task in minutes, -1 means unlimited\n",
    "            'maxRunTime': 60,  \n",
    "            # Max number of training job to submitted for hpo task, -1 means unlimited’,\n",
    "            'maxJobNum': 4,            \n",
    "            # Max number of training job to run in parallel, default 1. It depends on both the\n",
    "            # avaiable resource and if the search algorithm support to run in parallel, current only Random\n",
    "            # fully supports to run in parallel, Hyperband and Tpe supports to to in parellel in some phase,\n",
    "            # Bayesian runs in sequence now.\n",
    "            'maxParalleJobNum': 2, \n",
    "            # Name of the target metric that we are trying to optimize when searching hyper-parameters.\n",
    "            # It is the same metric name that the model update part 2 trying to dump.\n",
    "            'objectiveMetric' : 'loss',\n",
    "            # Strategy as how to optimize the hyper-parameters, minimize means to find better hyper-parameters to\n",
    "            # make the above objectiveMetric as small as possible, maximize means the opposite.\n",
    "            'objective' : 'minimize',\n",
    "        },\n",
    "    \n",
    "        # Define the hyper-paremeters to search and the corresponding search space.\n",
    "        'hyperParams':\n",
    "        [\n",
    "             {\n",
    "                 # Hyperparameter name, which will be the hyper-parameter key in config.json\n",
    "                 'name': 'learning_rate',\n",
    "                 # One of Range, Discrete\n",
    "                 'type': 'Range',\n",
    "                 # one of int, double, str\n",
    "                 'dataType': 'DOUBLE',\n",
    "                 # lower bound and upper bound when type=range and dataType=double\n",
    "                 'minDbVal': 0.001,\n",
    "                 'maxDbVal': 0.1,\n",
    "                 # lower bound and upper bound when type=range and dataType=int\n",
    "                 'minIntVal': 0,\n",
    "                 'maxIntVal': 0,\n",
    "                 # Discrete value list when type=discrete\n",
    "                 'discreteDbVal': [],\n",
    "                 'discreteIntVal': [],\n",
    "                 'discreateStrVal': []\n",
    "                 #step size to split the Range space. ONLY valid when type is Range\n",
    "                 #'step': '0.002',\n",
    "             }\n",
    "         ]\n",
    "    }\n",
    "mydata={'data':json.dumps(data)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit the Post request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit hpo task through the Post call and a hpo name/id as string format will get back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T14:48:07.857438Z",
     "start_time": "2020-05-22T14:48:07.727248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model submitted successfully: vanstee-hpo-485074186466936\n"
     ]
    }
   ],
   "source": [
    "startTuneUrl=dl_rest_url + '/hypersearch'\n",
    "r = req.post(startTuneUrl, headers=commonHeaders, data=mydata, files=files, verify=False, auth=myauth)\n",
    "\n",
    "if r.ok:\n",
    "    hpoName = r.json()\n",
    "    print ('\\nModel submitted successfully: {}'.format(hpoName))\n",
    "  \n",
    "else:\n",
    "    print('\\nModel submission failed with code={}, {}'. format(r.status_code, r.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T14:58:56.801359Z",
     "start_time": "2020-05-22T14:56:43.829Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "getHpoUrl = dl_rest_url +'/hypersearch/'+ hpoName\n",
    "\n",
    "res = req.get(getHpoUrl, headers=commonHeaders, verify=False, auth=myauth)\n",
    "if not res.ok:\n",
    "    print('get hpo task failed: code=%s, %s'%(res.status_code, res.content))\n",
    "else:\n",
    "    json_out=res.json()\n",
    "    \n",
    "    while json_out['state'] in ['SUBMITTED','RUNNING']:\n",
    "        IPython.display.clear_output()¶\n",
    "        print('Hpo task %s state %s progress %s%%'%(hpoName, json_out['state'], json_out['progress']))\n",
    "        time.sleep(1)\n",
    "        res = req.get(getHpoUrl, headers=commonHeaders, verify=False, auth=myauth)\n",
    "        json_out=res.json()\n",
    "        \n",
    "        experiments_length = len(json_out['experiments'])\n",
    "       \n",
    "        ####\n",
    "        ## Query the list of 6 sub-training of current batch, as maxParalleJobNum=6\n",
    "        ###      \n",
    "        count=0\n",
    "        Experiment = []\n",
    "        while (count < experiments_length):\n",
    "                appID = json_out['experiments'][count]['appId']\n",
    "                print ('appID: %s,' %appID )\n",
    "                print ('count: %d' %count)\n",
    "                Experiment.insert(count, appID)\n",
    "                count+=1\n",
    " \n",
    "        ####\n",
    "        ## Query the state of 6 sub-training of current batch\n",
    "        ###\n",
    "    \n",
    "        count = 0\n",
    "        while (count < len(Experiment)):\n",
    "                r = requests.get(dl_rest_url+'/execs/'+Experiment[count], auth=myauth, headers=commonHeaders, verify=False).json()    \n",
    "                if not res.ok:\n",
    "                    print('get hpo task failed: code=%s, %s'%(res.status_code, res.content))\n",
    "                else:\n",
    "                    print ('Experiement %s state: %s' %(Experiment[count], r['state']))\n",
    "                count+=1\n",
    "        \n",
    "        #time.sleep(30)\n",
    "        #print ('state:' + json_out['state'] )\n",
    "\n",
    "        \n",
    "print('Hpo task %s completes with state %s'%(hpoName, json_out['state']))\n",
    "print(json.dumps(json_out, indent=4, sort_keys=True))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
